{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"Exercicios/Dados/ex1_data/","title":"Exercicio 1 - Data","text":"In\u00a0[14]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nclass0 = {\n    \"x\": np.random.normal(2, 0.8, size=100),\n    \"y\": np.random.normal(3, 2.5, size=100)\n}\n\nclass1 = {\n    \"x\": np.random.normal(5, 1.2, size=100),\n    \"y\": np.random.normal(6, 1.9, size=100)\n}\n\nclass2 = {\n    \"x\": np.random.normal(8, 0.9, size=100),\n    \"y\": np.random.normal(1, 0.9, size=100)\n}\n\nclass3 = {\n    \"x\": np.random.normal(15, 0.5, size=100),\n    \"y\": np.random.normal(4, 2.0, size=100)\n}\n\n\nplt.figure(figsize=(8,6))\nplt.scatter(class0[\"x\"], class0[\"y\"], label=\"Class 0\", alpha=0.6)\nplt.scatter(class1[\"x\"], class1[\"y\"], label=\"Class 1\", alpha=0.6)\nplt.scatter(class2[\"x\"], class2[\"y\"], label=\"Class 2\", alpha=0.6)\nplt.scatter(class3[\"x\"], class3[\"y\"], label=\"Class 3\", alpha=0.6)\nplt.legend()\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Dados Sinteticos Aleat\u00f3rios\")\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42)  class0 = {     \"x\": np.random.normal(2, 0.8, size=100),     \"y\": np.random.normal(3, 2.5, size=100) }  class1 = {     \"x\": np.random.normal(5, 1.2, size=100),     \"y\": np.random.normal(6, 1.9, size=100) }  class2 = {     \"x\": np.random.normal(8, 0.9, size=100),     \"y\": np.random.normal(1, 0.9, size=100) }  class3 = {     \"x\": np.random.normal(15, 0.5, size=100),     \"y\": np.random.normal(4, 2.0, size=100) }   plt.figure(figsize=(8,6)) plt.scatter(class0[\"x\"], class0[\"y\"], label=\"Class 0\", alpha=0.6) plt.scatter(class1[\"x\"], class1[\"y\"], label=\"Class 1\", alpha=0.6) plt.scatter(class2[\"x\"], class2[\"y\"], label=\"Class 2\", alpha=0.6) plt.scatter(class3[\"x\"], class3[\"y\"], label=\"Class 3\", alpha=0.6) plt.legend() plt.xlabel(\"X1\") plt.ylabel(\"X2\") plt.title(\"Dados Sinteticos Aleat\u00f3rios\") plt.show()  <ol> <li>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</li> <li>Analyze and Draw Boundaries:<ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.<ul> <li>Class 0 and 1 have the most overlap, being pretty ditinguishable from the other two classes (numbers 2 and 3). The most segregated one is class 3, being at the far end of the X1 axis of the plot.</li> </ul> </li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?<ul> <li>I would argue that a line can be made to separate classes from each other, but the line would also put different classes on the same side. This would mean we would need at minimum a second line to properly separate all classes.</li> </ul> </li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> </li> </ol> In\u00a0[15]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nmu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nclass_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nclass_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack((class_A, class_B))\ny = np.array([0]*500 + [1]*500)\n\nprint(\"Dataset shape:\", X.shape)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"PCA de dados Sint\u00e9ticos 5D - Redu\u00e7\u00e3o para 2D\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA  np.random.seed(42)  mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  class_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500) class_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack((class_A, class_B)) y = np.array([0]*500 + [1]*500)  print(\"Dataset shape:\", X.shape)  pca = PCA(n_components=2) X_pca = pca.fit_transform(X)  plt.figure(figsize=(8,6)) plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], alpha=0.6, label=\"Class A\") plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"PCA de dados Sint\u00e9ticos 5D - Redu\u00e7\u00e3o para 2D\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") plt.legend() plt.show()  <pre>Dataset shape: (1000, 5)\n</pre> <ol> <li>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.<ul> <li>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.</li> <li>Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</li> </ul> </li> <li>Analyze the Plots:<ol> <li><p>Based on your 2D projection, describe the relationship between the two classes.</p> <ul> <li>Very intertwined, making it extremely difficult to separate them clearly. They do tend to different sides of the plot (in the x-axis), but they do have noticeable overlap.</li> </ul> </li> <li><p>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</p> <ul> <li>Creating a single line that segregates these two classes is not possible, given that they do have overlap. If you were to trace a line between them, it would end up inevitably classifying class A as class B and vice-versa.</li> </ul> </li> </ol> </li> </ol> In\u00a0[16]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"../../../data/SpaceshipTitanic/train.csv\")\n\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv(\"../../../data/SpaceshipTitanic/train.csv\")  df.head() Out[16]: PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported 0 0001_01 Europa False B/0/P TRAPPIST-1e 39.0 False 0.0 0.0 0.0 0.0 0.0 Maham Ofracculy False 1 0002_01 Earth False F/0/S TRAPPIST-1e 24.0 False 109.0 9.0 25.0 549.0 44.0 Juanna Vines True 2 0003_01 Europa False A/0/S TRAPPIST-1e 58.0 True 43.0 3576.0 0.0 6715.0 49.0 Altark Susent False 3 0003_02 Europa False A/0/S TRAPPIST-1e 33.0 False 0.0 1283.0 371.0 3329.0 193.0 Solam Susent False 4 0004_01 Earth False F/1/S TRAPPIST-1e 16.0 False 303.0 70.0 151.0 565.0 2.0 Willy Santantines True In\u00a0[17]: Copied! <pre>null_counts = df.isnull().sum()\n\nprint(\"Null values per column:\")\nprint(null_counts) \n</pre> null_counts = df.isnull().sum()  print(\"Null values per column:\") print(null_counts)  <pre>Null values per column:\nPassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\ndtype: int64\n</pre> <p>For these missing values, we must treat numerical and categorical features separately.</p> <p>For numerical features, we use a Simple Imputer to fill null values. In this instance, I chose the Median value to be used as filler. After that, we use a Standard Scaler to put all values centered at 0.</p> <p>For categorical features, we use the most frequent class as a fill-in for missing values, and pass it to a OneHotEncoder afterwards so our categorical values become boolean features. In this case, this gave us 26 features compared to our initial 14</p> In\u00a0[34]: Copied! <pre>from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\n\nX = df.drop([\"PassengerId\", \"Name\"], axis=1)\n\nprint(\"Original Data Without PassengerId and Name\")\nprint(X.shape)\n\nX[[\"Deck\", \"Num\", \"Side\"]] = X[\"Cabin\"].str.split(\"/\", expand=True)\nX.drop(\"Cabin\", axis=1, inplace=True)\n\nprint(\"\\nAfter splitting Cabin into Deck, Number and Side of ship (3 new columns)\")\nprint(X.shape)\n\nnum_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\n\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocessor = ColumnTransformer([\n    (\"num\", num_pipeline, num_features),\n    (\"cat\", cat_pipeline, cat_features)\n])\n\nX_processed = preprocessor.fit_transform(X)\n\nprint(\"\\nAfter Pipelines\")\nprint(X_processed.shape)\n</pre> from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline import pandas as pd   X = df.drop([\"PassengerId\", \"Name\"], axis=1)  print(\"Original Data Without PassengerId and Name\") print(X.shape)  X[[\"Deck\", \"Num\", \"Side\"]] = X[\"Cabin\"].str.split(\"/\", expand=True) X.drop(\"Cabin\", axis=1, inplace=True)  print(\"\\nAfter splitting Cabin into Deck, Number and Side of ship (3 new columns)\") print(X.shape)  num_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] cat_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]  num_pipeline = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"median\")),     (\"scaler\", StandardScaler()) ])  cat_pipeline = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")) ])  preprocessor = ColumnTransformer([     (\"num\", num_pipeline, num_features),     (\"cat\", cat_pipeline, cat_features) ])  X_processed = preprocessor.fit_transform(X)  print(\"\\nAfter Pipelines\") print(X_processed.shape) <pre>Original Data Without PassengerId and Name\n(8693, 12)\n\nAfter splitting Cabin into Deck, Number and Side of ship (3 new columns)\n(8693, 14)\n\nAfter Pipelines\n(8693, 26)\n</pre> <ol> <li>Preprocess the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.<ul> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ul> </li> <li>Visualize the Results:<ul> <li>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) before and after scaling to show the effect of your transformation.</li> </ul> </li> </ol> In\u00a0[35]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nfig, axes = plt.subplots(len(num_features), 2, figsize=(25, 4*len(num_features)))\n\nfor i in range(len(num_features)):\n    \n    feature = num_features[i]\n    \n    df[feature].hist(ax=axes[i, 0], bins=30, color=\"skyblue\")\n    axes[i, 0].set_title(f\"{feature} before scaling\")\n\n    scaled = StandardScaler().fit_transform(df[[feature]].fillna(df[feature].median()))\n    pd.Series(scaled.ravel()).hist(ax=axes[i, 1], bins=30, color=\"salmon\")\n    axes[i, 1].set_title(f\"{feature} after standardization\")\n\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.preprocessing import StandardScaler  fig, axes = plt.subplots(len(num_features), 2, figsize=(25, 4*len(num_features)))  for i in range(len(num_features)):          feature = num_features[i]          df[feature].hist(ax=axes[i, 0], bins=30, color=\"skyblue\")     axes[i, 0].set_title(f\"{feature} before scaling\")      scaled = StandardScaler().fit_transform(df[[feature]].fillna(df[feature].median()))     pd.Series(scaled.ravel()).hist(ax=axes[i, 1], bins=30, color=\"salmon\")     axes[i, 1].set_title(f\"{feature} after standardization\")  plt.tight_layout() plt.show()  <p>Notice that the shape of the graphs is maintained, but the values in the x-axis are centered to 0.</p>"},{"location":"Exercicios/Dados/ex1_data/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/Dados/ex1_data/#exploring-class-separability-in-2d","title":"Exploring Class Separability in 2D\u00b6","text":"<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/Dados/ex1_data/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:<ul> <li>Class 0: Mean = [2, 3], Standard Deviation = [0.8, 2.5]</li> <li>Class 1: Mean = [5, 6], Standard Deviation = [1.2, 1.9]</li> <li>Class 2: Mean = [8, 1], Standard Deviation = [0.9, 0.9]</li> <li>Class 3: Mean = [15, 4], Standard Deviation = [0.5, 2.0]</li> </ul> </li> </ol>"},{"location":"Exercicios/Dados/ex1_data/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/Dados/ex1_data/#non-linearity-in-higher-dimensions","title":"Non-Linearity in Higher Dimensions\u00b6","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/Dados/ex1_data/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B. Use a multivariate normal distribution with the following parameters:</p> <ul> <li><p>Class A:</p> <p>Mean vector:</p> <p>$$\\mu_A = [0, 0, 0, 0, 0]$$</p> <p>Covariance matrix:</p> <p>$$   \\Sigma_A = \\begin{pmatrix}   1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\   0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\   0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\   0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\   0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0   \\end{pmatrix}   $$</p> </li> <li><p>Class B:</p> <p>Mean vector:</p> <p>$$\\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5]$$</p> <p>Covariance matrix:</p> <p>$$   \\Sigma_B = \\begin{pmatrix}   1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\   -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\   0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\   0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\   0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5   \\end{pmatrix}   $$</p> </li> </ul> </li> </ol>"},{"location":"Exercicios/Dados/ex1_data/#exercise-3","title":"Exercise 3\u00b6","text":""},{"location":"Exercicios/Dados/ex1_data/#preparing-real-world-data-for-a-neural-network","title":"Preparing Real-World Data for a Neural Network\u00b6","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (<code>tanh</code>) activation function in its hidden layers.</p>"},{"location":"Exercicios/Dados/ex1_data/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Get the Data: Download the Spaceship Titanic dataset from Kaggle.</p> </li> <li><p>Describe the Data:</p> <ul> <li><p>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).</p> <ul> <li>We're trying to predict if a passenger was transported to another dimension during the crash of the Spaceship Titanic, the value that represents whether or not the passenger was transported is in the <code>Transported column</code></li> </ul> </li> <li><p>List the features and identify which are numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>).</p> </li> <li><p>Investigate the dataset for missing values. Which columns have them, and how many?</p> </li> </ul> </li> </ol> <p>Categorical Columns -</p> <ul> <li>HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.</li> <li>Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</li> <li>Destination - The planet the passenger will be debarking to.</li> <li>Name - The first and last names of the passenger.</li> <li>PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</li> </ul> <p>Boolean Columns -</p> <ul> <li>Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.</li> <li>VIP - Whether the passenger has paid for special VIP service during the voyage.</li> <li>CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</li> </ul> <p>Numerical Columns -</p> <ul> <li>Age - The age of the passenger.</li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.</li> </ul>"},{"location":"Exercicios/Perceptron/ex2_perceptron/","title":"Exercicio 2 - Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nmu_A = [2, 2]\nSigma_A = np.array([\n    [0.5, 0.0],\n    [0.0, 0.5]\n])\n\nmu_B = [5, 5]\nSigma_B = np.array([\n    [0.5, 0.0],\n    [0.0, 0.5]\n])\n\nclass_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nclass_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack((class_A, class_B))\ny = np.array([-1]*500 + [1]*500)\n\nprint(\"Dataset shape:\", X.shape)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA  np.random.seed(42)  mu_A = [2, 2] Sigma_A = np.array([     [0.5, 0.0],     [0.0, 0.5] ])  mu_B = [5, 5] Sigma_B = np.array([     [0.5, 0.0],     [0.0, 0.5] ])  class_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500) class_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack((class_A, class_B)) y = np.array([-1]*500 + [1]*500)  print(\"Dataset shape:\", X.shape)  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") plt.legend() plt.show()  <pre>Dataset shape: (1000, 2)\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import accuracy_score\n\nw = np.zeros(2, dtype=float)\nb = 0\nalpha = 0.01\n\ndef activation(scores):\n    return np.where(scores &gt;= 0, 1, -1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        score = np.dot(w, x_i) + b\n        \n        y_pred = activation(score)\n        \n        error = y_i - y_pred\n        \n        if error != 0:\n            w += alpha * error * x_i \n            b += alpha * error\n            wrong = True\n\n    if not wrong:            \n        break\n\nscores = X @ w + b\ny_pred = activation(scores)\n\nprint(\"Final weights:\", w, \"bias:\", b)\nprint(\"Accuracy:\", accuracy_score(y, y_pred))\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nx_plot = np.linspace(0, 10, 100)\ny_plot = -(w[0]/w[1])*x_plot - b/w[1]\nplt.plot(x_plot, y_plot, 'r-')\nplt.legend()\nplt.show()\n</pre> import numpy as np from sklearn.metrics import accuracy_score  w = np.zeros(2, dtype=float) b = 0 alpha = 0.01  def activation(scores):     return np.where(scores &gt;= 0, 1, -1)  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          score = np.dot(w, x_i) + b                  y_pred = activation(score)                  error = y_i - y_pred                  if error != 0:             w += alpha * error * x_i              b += alpha * error             wrong = True      if not wrong:                     break  scores = X @ w + b y_pred = activation(scores)  print(\"Final weights:\", w, \"bias:\", b) print(\"Accuracy:\", accuracy_score(y, y_pred))  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") x_plot = np.linspace(0, 10, 100) y_plot = -(w[0]/w[1])*x_plot - b/w[1] plt.plot(x_plot, y_plot, 'r-') plt.legend() plt.show() <pre>Final weights: [0.34065515 0.15323602] bias: -1.4200000000000008\nAccuracy: 0.975\n</pre> <p>The data is well separated, meaning we can divide the two classes with a straight line. Fixing the weights and bias doesn't take long because of the nature of the data</p> In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nmu_A = [3, 3]\nSigma_A = np.array([\n    [1.5, 0.0],\n    [0.0, 1.5]\n])\n\nmu_B = [4, 4]\nSigma_B = np.array([\n    [1.5, 0.0],\n    [0.0, 1.5]\n])\n\nclass_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nclass_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack((class_A, class_B))\ny = np.array([-1]*500 + [1]*500)\n\nprint(\"Dataset shape:\", X.shape)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA  np.random.seed(42)  mu_A = [3, 3] Sigma_A = np.array([     [1.5, 0.0],     [0.0, 1.5] ])  mu_B = [4, 4] Sigma_B = np.array([     [1.5, 0.0],     [0.0, 1.5] ])  class_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500) class_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack((class_A, class_B)) y = np.array([-1]*500 + [1]*500)  print(\"Dataset shape:\", X.shape)  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") plt.legend() plt.show()  <pre>Dataset shape: (1000, 2)\n</pre> In\u00a0[4]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import accuracy_score\n\nw = np.zeros(2, dtype=float)\nb = 0\nalpha = 0.01\n\ndef activation(scores):\n    return np.where(scores &gt;= 0, 1, -1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        score = np.dot(w, x_i) + b\n        \n        y_pred = activation(score)\n        \n        error = y_i - y_pred\n        \n        if error != 0:\n            w += alpha * error * x_i \n            b += alpha * error\n            wrong = True\n\n    if not wrong:            \n        break\n\nscores = X @ w + b\ny_pred = activation(scores)\n\nprint(\"Final weights:\", w, \"bias:\", b)\nprint(\"Accuracy:\", accuracy_score(y, y_pred))\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nx_plot = np.linspace(0, 10, 100)\ny_plot = -(w[0]/w[1])*x_plot - b/w[1]\nplt.plot(x_plot, y_plot, 'r-')\nplt.legend()\nplt.show()\n</pre> import numpy as np from sklearn.metrics import accuracy_score  w = np.zeros(2, dtype=float) b = 0 alpha = 0.01  def activation(scores):     return np.where(scores &gt;= 0, 1, -1)  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          score = np.dot(w, x_i) + b                  y_pred = activation(score)                  error = y_i - y_pred                  if error != 0:             w += alpha * error * x_i              b += alpha * error             wrong = True      if not wrong:                     break  scores = X @ w + b y_pred = activation(scores)  print(\"Final weights:\", w, \"bias:\", b) print(\"Accuracy:\", accuracy_score(y, y_pred))  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") x_plot = np.linspace(0, 10, 100) y_plot = -(w[0]/w[1])*x_plot - b/w[1] plt.plot(x_plot, y_plot, 'r-') plt.legend() plt.show() <pre>Final weights: [0.18071183 0.10106285] bias: -0.36000000000000004\nAccuracy: 0.509\n</pre> <p>The data isn't as well separated as in the first exercise. This is made clear by the fact that the classes haev overlap, while in the previous situation they were already separated. This means that, for these two classes, a linear division isn't possible in the way that we're implementing.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/Perceptron/ex2_perceptron/#data-generation-task","title":"Data Generation Task:\u00b6","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <p>Mean =</p> <p>$[2, 2]$</p> <p>Covariance matrix =</p> <p>$[[0.5, 0], [0, 0.5]]$</p> <p>(i.e., variance of $0.5$ along each dimension, no covariance).</p> </li> <li><p>Class 1:</p> <p>Mean =</p> <p>$[5, 5]$</p> <p>Covariance matrix =</p> <p>$[[0.5, 0], [0, 0.5]]$</p> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#perceptron-implementation-task","title":"Perceptron Implementation Task:\u00b6","text":"<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).</li> <li>Use the perceptron learning rule: For each misclassified sample $(x, y)$, update $w = w + \u03b7 * y * x$ and $b = b + \u03b7 * y$, where $\u03b7$ is the learning rate (start with $\u03b7=0.01$).</li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</li> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by $w\u00b7x + b = 0$) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/Perceptron/ex2_perceptron/#data-generation-task","title":"Data Generation Task:\u00b6","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <p>Mean =</p> <p>$[3, 3]$</p> <p>Covariance matrix =</p> <p>$[[1.5, 0], [0, 1.5]]$</p> </li> <li><p>Class 1:</p> <p>Mean =</p> <p>$[4, 4]$</p> <p>Covariance matrix =</p> <p>$[[1.5, 0], [0, 1.5]]$</p> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#perceptron-implementation-task","title":"Perceptron Implementation Task:\u00b6","text":"<p>Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.</p> <ul> <li>Follow the same initialization, update rule, and training process.</li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p>"}]}