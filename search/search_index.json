{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"Exercicios/Dados/ex1_data/","title":"Exercicio 1 - Data","text":"In\u00a0[14]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nclass0 = {\n    \"x\": np.random.normal(2, 0.8, size=100),\n    \"y\": np.random.normal(3, 2.5, size=100)\n}\n\nclass1 = {\n    \"x\": np.random.normal(5, 1.2, size=100),\n    \"y\": np.random.normal(6, 1.9, size=100)\n}\n\nclass2 = {\n    \"x\": np.random.normal(8, 0.9, size=100),\n    \"y\": np.random.normal(1, 0.9, size=100)\n}\n\nclass3 = {\n    \"x\": np.random.normal(15, 0.5, size=100),\n    \"y\": np.random.normal(4, 2.0, size=100)\n}\n\n\nplt.figure(figsize=(8,6))\nplt.scatter(class0[\"x\"], class0[\"y\"], label=\"Class 0\", alpha=0.6)\nplt.scatter(class1[\"x\"], class1[\"y\"], label=\"Class 1\", alpha=0.6)\nplt.scatter(class2[\"x\"], class2[\"y\"], label=\"Class 2\", alpha=0.6)\nplt.scatter(class3[\"x\"], class3[\"y\"], label=\"Class 3\", alpha=0.6)\nplt.legend()\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Dados Sinteticos Aleat\u00f3rios\")\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42)  class0 = {     \"x\": np.random.normal(2, 0.8, size=100),     \"y\": np.random.normal(3, 2.5, size=100) }  class1 = {     \"x\": np.random.normal(5, 1.2, size=100),     \"y\": np.random.normal(6, 1.9, size=100) }  class2 = {     \"x\": np.random.normal(8, 0.9, size=100),     \"y\": np.random.normal(1, 0.9, size=100) }  class3 = {     \"x\": np.random.normal(15, 0.5, size=100),     \"y\": np.random.normal(4, 2.0, size=100) }   plt.figure(figsize=(8,6)) plt.scatter(class0[\"x\"], class0[\"y\"], label=\"Class 0\", alpha=0.6) plt.scatter(class1[\"x\"], class1[\"y\"], label=\"Class 1\", alpha=0.6) plt.scatter(class2[\"x\"], class2[\"y\"], label=\"Class 2\", alpha=0.6) plt.scatter(class3[\"x\"], class3[\"y\"], label=\"Class 3\", alpha=0.6) plt.legend() plt.xlabel(\"X1\") plt.ylabel(\"X2\") plt.title(\"Dados Sinteticos Aleat\u00f3rios\") plt.show()  <ol> <li><p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p> </li> <li><p>Analyze and Draw Boundaries:</p> <ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.<ul> <li>Class 0 and 1 have the most overlap, being pretty ditinguishable from the other two classes (numbers 2 and 3). The most segregated one is class 3, being at the far end of the X1 axis of the plot.</li> </ul> </li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?<ul> <li>I would argue that a line can be made to separate classes from each other, but the line would also put different classes on the same side. This would mean we would need at minimum a second line to properly separate all classes.</li> </ul> </li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> <p> </p> </li> </ol> In\u00a0[15]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nmu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nclass_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nclass_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack((class_A, class_B))\ny = np.array([0]*500 + [1]*500)\n\nprint(\"Dataset shape:\", X.shape)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"PCA de dados Sint\u00e9ticos 5D - Redu\u00e7\u00e3o para 2D\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA  np.random.seed(42)  mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  class_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500) class_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack((class_A, class_B)) y = np.array([0]*500 + [1]*500)  print(\"Dataset shape:\", X.shape)  pca = PCA(n_components=2) X_pca = pca.fit_transform(X)  plt.figure(figsize=(8,6)) plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], alpha=0.6, label=\"Class A\") plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"PCA de dados Sint\u00e9ticos 5D - Redu\u00e7\u00e3o para 2D\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") plt.legend() plt.show()  <pre>Dataset shape: (1000, 5)\n</pre> <ol> <li>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.<ul> <li>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.</li> <li>Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</li> </ul> </li> <li>Analyze the Plots:<ol> <li><p>Based on your 2D projection, describe the relationship between the two classes.</p> <ul> <li>Very intertwined, making it extremely difficult to separate them clearly. They do tend to different sides of the plot (in the x-axis), but they do have noticeable overlap.</li> </ul> </li> <li><p>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</p> <ul> <li>Creating a single line that segregates these two classes is not possible, given that they do have overlap. If you were to trace a line between them, it would end up inevitably classifying class A as class B and vice-versa.</li> </ul> </li> </ol> </li> </ol> <p> </p> In\u00a0[16]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"../../../data/SpaceshipTitanic/train.csv\")\n\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv(\"../../../data/SpaceshipTitanic/train.csv\")  df.head() Out[16]: PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported 0 0001_01 Europa False B/0/P TRAPPIST-1e 39.0 False 0.0 0.0 0.0 0.0 0.0 Maham Ofracculy False 1 0002_01 Earth False F/0/S TRAPPIST-1e 24.0 False 109.0 9.0 25.0 549.0 44.0 Juanna Vines True 2 0003_01 Europa False A/0/S TRAPPIST-1e 58.0 True 43.0 3576.0 0.0 6715.0 49.0 Altark Susent False 3 0003_02 Europa False A/0/S TRAPPIST-1e 33.0 False 0.0 1283.0 371.0 3329.0 193.0 Solam Susent False 4 0004_01 Earth False F/1/S TRAPPIST-1e 16.0 False 303.0 70.0 151.0 565.0 2.0 Willy Santantines True In\u00a0[17]: Copied! <pre>null_counts = df.isnull().sum()\n\nprint(\"Null values per column:\")\nprint(null_counts) \n</pre> null_counts = df.isnull().sum()  print(\"Null values per column:\") print(null_counts)  <pre>Null values per column:\nPassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\ndtype: int64\n</pre> <p>For these missing values, we must treat numerical and categorical features separately.</p> <p>For numerical features, we use a Simple Imputer to fill null values. In this instance, I chose the Median value to be used as filler. After that, we use a Standard Scaler to put all values centered at 0.</p> <p>For categorical features, we use the most frequent class as a fill-in for missing values, and pass it to a OneHotEncoder afterwards so our categorical values become boolean features. In this case, this gave us 26 features compared to our initial 14</p> In\u00a0[34]: Copied! <pre>from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\n\nX = df.drop([\"PassengerId\", \"Name\"], axis=1)\n\nprint(\"Original Data Without PassengerId and Name\")\nprint(X.shape)\n\nX[[\"Deck\", \"Num\", \"Side\"]] = X[\"Cabin\"].str.split(\"/\", expand=True)\nX.drop(\"Cabin\", axis=1, inplace=True)\n\nprint(\"\\nAfter splitting Cabin into Deck, Number and Side of ship (3 new columns)\")\nprint(X.shape)\n\nnum_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\n\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocessor = ColumnTransformer([\n    (\"num\", num_pipeline, num_features),\n    (\"cat\", cat_pipeline, cat_features)\n])\n\nX_processed = preprocessor.fit_transform(X)\n\nprint(\"\\nAfter Pipelines\")\nprint(X_processed.shape)\n</pre> from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline import pandas as pd   X = df.drop([\"PassengerId\", \"Name\"], axis=1)  print(\"Original Data Without PassengerId and Name\") print(X.shape)  X[[\"Deck\", \"Num\", \"Side\"]] = X[\"Cabin\"].str.split(\"/\", expand=True) X.drop(\"Cabin\", axis=1, inplace=True)  print(\"\\nAfter splitting Cabin into Deck, Number and Side of ship (3 new columns)\") print(X.shape)  num_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] cat_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]  num_pipeline = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"median\")),     (\"scaler\", StandardScaler()) ])  cat_pipeline = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")) ])  preprocessor = ColumnTransformer([     (\"num\", num_pipeline, num_features),     (\"cat\", cat_pipeline, cat_features) ])  X_processed = preprocessor.fit_transform(X)  print(\"\\nAfter Pipelines\") print(X_processed.shape) <pre>Original Data Without PassengerId and Name\n(8693, 12)\n\nAfter splitting Cabin into Deck, Number and Side of ship (3 new columns)\n(8693, 14)\n\nAfter Pipelines\n(8693, 26)\n</pre> <ol> <li>Preprocess the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.<ul> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ul> </li> <li>Visualize the Results:<ul> <li>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) before and after scaling to show the effect of your transformation.</li> </ul> </li> </ol> In\u00a0[35]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nfig, axes = plt.subplots(len(num_features), 2, figsize=(25, 4*len(num_features)))\n\nfor i in range(len(num_features)):\n    \n    feature = num_features[i]\n    \n    df[feature].hist(ax=axes[i, 0], bins=30, color=\"skyblue\")\n    axes[i, 0].set_title(f\"{feature} before scaling\")\n\n    scaled = StandardScaler().fit_transform(df[[feature]].fillna(df[feature].median()))\n    pd.Series(scaled.ravel()).hist(ax=axes[i, 1], bins=30, color=\"salmon\")\n    axes[i, 1].set_title(f\"{feature} after standardization\")\n\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.preprocessing import StandardScaler  fig, axes = plt.subplots(len(num_features), 2, figsize=(25, 4*len(num_features)))  for i in range(len(num_features)):          feature = num_features[i]          df[feature].hist(ax=axes[i, 0], bins=30, color=\"skyblue\")     axes[i, 0].set_title(f\"{feature} before scaling\")      scaled = StandardScaler().fit_transform(df[[feature]].fillna(df[feature].median()))     pd.Series(scaled.ravel()).hist(ax=axes[i, 1], bins=30, color=\"salmon\")     axes[i, 1].set_title(f\"{feature} after standardization\")  plt.tight_layout() plt.show()  <p>Notice that the shape of the graphs is maintained, but the values in the x-axis are centered to 0.</p>"},{"location":"Exercicios/Dados/ex1_data/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/Dados/ex1_data/#exploring-class-separability-in-2d","title":"Exploring Class Separability in 2D\u00b6","text":"<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/Dados/ex1_data/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:<ul> <li>Class 0: Mean = [2, 3], Standard Deviation = [0.8, 2.5]</li> <li>Class 1: Mean = [5, 6], Standard Deviation = [1.2, 1.9]</li> <li>Class 2: Mean = [8, 1], Standard Deviation = [0.9, 0.9]</li> <li>Class 3: Mean = [15, 4], Standard Deviation = [0.5, 2.0]</li> </ul> </li> </ol>"},{"location":"Exercicios/Dados/ex1_data/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/Dados/ex1_data/#non-linearity-in-higher-dimensions","title":"Non-Linearity in Higher Dimensions\u00b6","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/Dados/ex1_data/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B. Use a multivariate normal distribution with the following parameters:</p> <ul> <li><p>Class A:</p> <p>Mean vector:</p> <p>$$\\mu_A = [0, 0, 0, 0, 0]$$</p> <p>Covariance matrix:</p> <p>$$   \\Sigma_A = \\begin{pmatrix}   1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\   0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\   0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\   0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\   0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0   \\end{pmatrix}   $$</p> </li> <li><p>Class B:</p> <p>Mean vector:</p> <p>$$\\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5]$$</p> <p>Covariance matrix:</p> <p>$$   \\Sigma_B = \\begin{pmatrix}   1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\   -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\   0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\   0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\   0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5   \\end{pmatrix}   $$</p> </li> </ul> </li> </ol>"},{"location":"Exercicios/Dados/ex1_data/#exercise-3","title":"Exercise 3\u00b6","text":""},{"location":"Exercicios/Dados/ex1_data/#preparing-real-world-data-for-a-neural-network","title":"Preparing Real-World Data for a Neural Network\u00b6","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (<code>tanh</code>) activation function in its hidden layers.</p>"},{"location":"Exercicios/Dados/ex1_data/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Get the Data: Download the Spaceship Titanic dataset from Kaggle.</p> </li> <li><p>Describe the Data:</p> <ul> <li><p>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).</p> <ul> <li>We're trying to predict if a passenger was transported to another dimension during the crash of the Spaceship Titanic, the value that represents whether or not the passenger was transported is in the <code>Transported column</code></li> </ul> </li> <li><p>List the features and identify which are numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>).</p> </li> <li><p>Investigate the dataset for missing values. Which columns have them, and how many?</p> </li> </ul> </li> </ol> <p>Categorical Columns -</p> <ul> <li>HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.</li> <li>Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</li> <li>Destination - The planet the passenger will be debarking to.</li> <li>Name - The first and last names of the passenger.</li> <li>PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</li> </ul> <p>Boolean Columns -</p> <ul> <li>Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.</li> <li>VIP - Whether the passenger has paid for special VIP service during the voyage.</li> <li>CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</li> </ul> <p>Numerical Columns -</p> <ul> <li>Age - The age of the passenger.</li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.</li> </ul>"},{"location":"Exercicios/MLP/ex3_MLP/","title":"Exercicio 3 - Multi-Layer Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\nnp.set_printoptions(precision=8, suppress=False)\n\n# ----- Given values -----\nx  = np.array([0.5, -0.2])                # input\ny  = 1.0                                   # target\n\nW1 = np.array([[0.3, -0.1],                # hidden layer weights (2x2)\n               [0.2,  0.4]])\nb1 = np.array([0.1, -0.2])                 # hidden biases (2,)\n\nW2 = np.array([0.5, -0.3])                 # output layer weights (2,)\nb2 = 0.2                                   # output bias (scalar)\n\neta = 0.1                                   # learning rate for the update step\ntanh = np.tanh\ntanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n\n# ----- 1) Forward pass -----\nz1 = W1 @ x + b1            # pre-activations hidden (2,)\na1 = tanh(z1)               # activations hidden (2,)\nz2 = W2 @ a1 + b2           # pre-activation output (scalar)\ny_hat = tanh(z2)            # final output (scalar)\n\nprint(\"z^(1) =\", z1)\nprint(\"a^(1) =\", a1)\nprint(\"z^(2) =\", z2)\nprint(\"y_hat =\", y_hat)\n\n\n# ----- 2) Loss (MSE, N=1) -----\nL = (y - y_hat)**2\nprint(\"Loss L =\", L)\n\n\n# ----- 3) Backward pass -----\n# dL/dy_hat\ndL_dyhat = 2.0 * (y_hat - y)              # since N=1\n# dL/dz2\ndL_dz2 = dL_dyhat * tanhp(z2)\n\n# Output layer grads\n# dL/dW2 = dL/dz2 * a1\ndL_dW2 = dL_dz2 * a1\n# dL/db2 = dL/dz2\ndL_db2 = dL_dz2\n\n# Backprop to hidden\n# dL/da1 = dL/dz2 * W2\ndL_da1 = dL_dz2 * W2\n# dL/dz1 = dL/da1 \u2299 tanh'(z1)\ndL_dz1 = dL_da1 * tanhp(z1)\n\n# Hidden layer grads\n# dL/dW1 = (dL/dz1)[:, None] @ x[None, :]\ndL_dW1 = np.outer(dL_dz1, x)\n# dL/db1 = dL/dz1\ndL_db1 = dL_dz1\n\nprint(\"\\ndL/dy_hat =\", dL_dyhat)\nprint(\"dL/dz^(2) =\", dL_dz2)\nprint(\"dL/dW^(2) =\", dL_dW2)\nprint(\"dL/db^(2) =\", dL_db2)\nprint(\"dL/da^(1) =\", dL_da1)\nprint(\"dL/dz^(1) =\", dL_dz1)\nprint(\"dL/dW^(1) =\\n\", dL_dW1)\nprint(\"dL/db^(1) =\", dL_db1)\n\n\n# ----- 4) Parameter update (gradient descent, eta = 0.1) -----\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\nprint(\"\\nUpdated parameters:\")\nprint(\"W^(2)_new =\", W2_new)\nprint(\"b^(2)_new =\", b2_new)\nprint(\"W^(1)_new =\\n\", W1_new)\nprint(\"b^(1)_new =\", b1_new)\n</pre> import numpy as np  np.set_printoptions(precision=8, suppress=False)  # ----- Given values ----- x  = np.array([0.5, -0.2])                # input y  = 1.0                                   # target  W1 = np.array([[0.3, -0.1],                # hidden layer weights (2x2)                [0.2,  0.4]]) b1 = np.array([0.1, -0.2])                 # hidden biases (2,)  W2 = np.array([0.5, -0.3])                 # output layer weights (2,) b2 = 0.2                                   # output bias (scalar)  eta = 0.1                                   # learning rate for the update step tanh = np.tanh tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh  # ----- 1) Forward pass ----- z1 = W1 @ x + b1            # pre-activations hidden (2,) a1 = tanh(z1)               # activations hidden (2,) z2 = W2 @ a1 + b2           # pre-activation output (scalar) y_hat = tanh(z2)            # final output (scalar)  print(\"z^(1) =\", z1) print(\"a^(1) =\", a1) print(\"z^(2) =\", z2) print(\"y_hat =\", y_hat)   # ----- 2) Loss (MSE, N=1) ----- L = (y - y_hat)**2 print(\"Loss L =\", L)   # ----- 3) Backward pass ----- # dL/dy_hat dL_dyhat = 2.0 * (y_hat - y)              # since N=1 # dL/dz2 dL_dz2 = dL_dyhat * tanhp(z2)  # Output layer grads # dL/dW2 = dL/dz2 * a1 dL_dW2 = dL_dz2 * a1 # dL/db2 = dL/dz2 dL_db2 = dL_dz2  # Backprop to hidden # dL/da1 = dL/dz2 * W2 dL_da1 = dL_dz2 * W2 # dL/dz1 = dL/da1 \u2299 tanh'(z1) dL_dz1 = dL_da1 * tanhp(z1)  # Hidden layer grads # dL/dW1 = (dL/dz1)[:, None] @ x[None, :] dL_dW1 = np.outer(dL_dz1, x) # dL/db1 = dL/dz1 dL_db1 = dL_dz1  print(\"\\ndL/dy_hat =\", dL_dyhat) print(\"dL/dz^(2) =\", dL_dz2) print(\"dL/dW^(2) =\", dL_dW2) print(\"dL/db^(2) =\", dL_db2) print(\"dL/da^(1) =\", dL_da1) print(\"dL/dz^(1) =\", dL_dz1) print(\"dL/dW^(1) =\\n\", dL_dW1) print(\"dL/db^(1) =\", dL_db1)   # ----- 4) Parameter update (gradient descent, eta = 0.1) ----- W2_new = W2 - eta * dL_dW2 b2_new = b2 - eta * dL_db2 W1_new = W1 - eta * dL_dW1 b1_new = b1 - eta * dL_db1  print(\"\\nUpdated parameters:\") print(\"W^(2)_new =\", W2_new) print(\"b^(2)_new =\", b2_new) print(\"W^(1)_new =\\n\", W1_new) print(\"b^(1)_new =\", b1_new)  <pre>z^(1) = [ 0.27 -0.18]\na^(1) = [ 0.26362484 -0.17808087]\nz^(2) = 0.38523667817130075\ny_hat = 0.36724656264510797\nLoss L = 0.4003769124844312\n\ndL/dy_hat = -1.265506874709784\ndL/dz^(2) = -1.0948279147135995\ndL/dW^(2) = [-0.28862383  0.19496791]\ndL/db^(2) = -1.0948279147135995\ndL/da^(1) = [-0.54741396  0.32844837]\ndL/dz^(1) = [-0.50936975  0.31803236]\ndL/dW^(1) =\n [[-0.25468488  0.10187395]\n [ 0.15901618 -0.06360647]]\ndL/db^(1) = [-0.50936975  0.31803236]\n\nUpdated parameters:\nW^(2)_new = [ 0.52886238 -0.31949679]\nb^(2)_new = 0.30948279147136\nW^(1)_new =\n [[ 0.32546849 -0.1101874 ]\n [ 0.18409838  0.40636065]]\nb^(1)_new = [ 0.15093698 -0.23180324]\n</pre> In\u00a0[21]: Copied! <pre>import numpy as np\nfrom sklearn.datasets import make_classification\n\n# Reproducibility\nrng = np.random.RandomState(42)\n\ndef biased_weights_for(target_class, K=2, high=0.8):\n    \"\"\"\n    Return a weight vector of length K that heavily favors target_class.\n    Helps ensure enough samples of the desired class per call.\n    \"\"\"\n    low = (1.0 - high) / (K - 1)\n    w = np.full(K, low, dtype=float)\n    w[target_class] = high\n    return w\n\ndef sample_class_subset(\n    n_needed: int,\n    target_class: int,\n    n_clusters_per_class: int,\n    seed: int,\n    *,\n    n_features: int = 2,\n    n_informative: int = 2,\n    n_redundant: int = 0,\n    class_sep: float = 1.5,\n    flip_y: float = 0.0,\n    max_tries: int = 20,\n    K: int = 2\n):\n    \"\"\"\n    Generate samples with make_classification and keep only rows of 'target_class'.\n    We over-generate with biased 'weights' so we can downsample exactly n_needed.\n    \"\"\"\n    tries = 0\n    local_seed = seed\n    # Over-generate to boost the chance of hitting n_needed for target_class\n    n_generate = max(4 * n_needed, 2000)\n\n    while tries &lt; max_tries:\n        X_tmp, y_tmp = make_classification(\n            n_samples=n_generate,\n            n_features=n_features,\n            n_informative=n_informative,\n            n_redundant=n_redundant,\n            n_repeated=0,\n            n_classes=K,\n            n_clusters_per_class=n_clusters_per_class,\n            class_sep=class_sep,\n            flip_y=flip_y,\n            weights=biased_weights_for(target_class, K=K, high=0.8),\n            random_state=local_seed,\n        )\n\n        idx = np.flatnonzero(y_tmp == target_class)\n        if idx.size &gt;= n_needed:\n            chosen = rng.choice(idx, size=n_needed, replace=False)\n            return X_tmp[chosen], np.full(n_needed, target_class, dtype=int)\n\n        # Try again with a different seed\n        tries += 1\n        local_seed += 1\n\n    raise RuntimeError(\n        f\"Could not obtain {n_needed} samples for class={target_class} \"\n        f\"with n_clusters_per_class={n_clusters_per_class} after {max_tries} tries.\"\n    )\n\n# ---------- Build the asymmetric dataset ----------\nN = 1000\nK = 2\nn_per_class = [N // K] * K\nn_per_class[0] += N - sum(n_per_class)  # handle remainder if any (keeps total = N)\n\n# Assign distinct cluster counts per class\nclusters_per_class = {\n    0: 1,  # class 0 -&gt; 1 clusters\n    1: 2,  # class 1 -&gt; 2 clusters\n}\n\n# Different seeds per class for variety\nbase_seeds = {0: 42, 1: 1337}\n\nXs = []\nys = []\nfor c in range(K):\n    Xi, yi = sample_class_subset(\n        n_needed=n_per_class[c],\n        target_class=c,\n        n_clusters_per_class=clusters_per_class[c],\n        seed=base_seeds[c],\n        n_features=2,\n        n_informative=2,\n        n_redundant=0,\n        class_sep=1.6,   # tweak for difficulty vs. separability\n        flip_y=0.0,\n        K=K,\n    )\n    Xs.append(Xi)\n    ys.append(yi)\n\n# Combine and shuffle\nX = np.vstack(Xs)\ny = np.concatenate(ys)\nperm = rng.permutation(len(y))\nX = X[perm]\ny = y[perm]\n\nprint(\"X shape:\", X.shape, \"y shape:\", y.shape)\nprint(\"Class counts:\", np.bincount(y))\n# Expect: (1500, 4) and roughly balanced counts (exactly 500 each by construction)\n</pre> import numpy as np from sklearn.datasets import make_classification  # Reproducibility rng = np.random.RandomState(42)  def biased_weights_for(target_class, K=2, high=0.8):     \"\"\"     Return a weight vector of length K that heavily favors target_class.     Helps ensure enough samples of the desired class per call.     \"\"\"     low = (1.0 - high) / (K - 1)     w = np.full(K, low, dtype=float)     w[target_class] = high     return w  def sample_class_subset(     n_needed: int,     target_class: int,     n_clusters_per_class: int,     seed: int,     *,     n_features: int = 2,     n_informative: int = 2,     n_redundant: int = 0,     class_sep: float = 1.5,     flip_y: float = 0.0,     max_tries: int = 20,     K: int = 2 ):     \"\"\"     Generate samples with make_classification and keep only rows of 'target_class'.     We over-generate with biased 'weights' so we can downsample exactly n_needed.     \"\"\"     tries = 0     local_seed = seed     # Over-generate to boost the chance of hitting n_needed for target_class     n_generate = max(4 * n_needed, 2000)      while tries &lt; max_tries:         X_tmp, y_tmp = make_classification(             n_samples=n_generate,             n_features=n_features,             n_informative=n_informative,             n_redundant=n_redundant,             n_repeated=0,             n_classes=K,             n_clusters_per_class=n_clusters_per_class,             class_sep=class_sep,             flip_y=flip_y,             weights=biased_weights_for(target_class, K=K, high=0.8),             random_state=local_seed,         )          idx = np.flatnonzero(y_tmp == target_class)         if idx.size &gt;= n_needed:             chosen = rng.choice(idx, size=n_needed, replace=False)             return X_tmp[chosen], np.full(n_needed, target_class, dtype=int)          # Try again with a different seed         tries += 1         local_seed += 1      raise RuntimeError(         f\"Could not obtain {n_needed} samples for class={target_class} \"         f\"with n_clusters_per_class={n_clusters_per_class} after {max_tries} tries.\"     )  # ---------- Build the asymmetric dataset ---------- N = 1000 K = 2 n_per_class = [N // K] * K n_per_class[0] += N - sum(n_per_class)  # handle remainder if any (keeps total = N)  # Assign distinct cluster counts per class clusters_per_class = {     0: 1,  # class 0 -&gt; 1 clusters     1: 2,  # class 1 -&gt; 2 clusters }  # Different seeds per class for variety base_seeds = {0: 42, 1: 1337}  Xs = [] ys = [] for c in range(K):     Xi, yi = sample_class_subset(         n_needed=n_per_class[c],         target_class=c,         n_clusters_per_class=clusters_per_class[c],         seed=base_seeds[c],         n_features=2,         n_informative=2,         n_redundant=0,         class_sep=1.6,   # tweak for difficulty vs. separability         flip_y=0.0,         K=K,     )     Xs.append(Xi)     ys.append(yi)  # Combine and shuffle X = np.vstack(Xs) y = np.concatenate(ys) perm = rng.permutation(len(y)) X = X[perm] y = y[perm]  print(\"X shape:\", X.shape, \"y shape:\", y.shape) print(\"Class counts:\", np.bincount(y)) # Expect: (1500, 4) and roughly balanced counts (exactly 500 each by construction)  <pre>X shape: (1000, 2) y shape: (1000,)\nClass counts: [500 500]\n</pre> In\u00a0[22]: Copied! <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n\nclass HiddenLayer():\n    \n    def __init__(self, w_size, b_size):\n        self.W = np.random.uniform(-1, 1, size=w_size)\n        self.b = np.random.uniform(-1, 1, size=b_size)\n        self.z = 0                # pre-activation value\n        self.a = 0                # activation value \n        self.prev_a = 0\n\n    def set_z(self, z):\n        self.z = z\n        \n    def set_a(self, a):\n        self.a = a\n    \n    def set_W(self, W):\n        self.W = W    \n        \n    def set_b(self, b):\n        self.b = b\n        \n    def set_prev_a(self, value):\n        self.prev_a = value\n        \n        \n    def update_W(self, eta, dW):\n        self.W -= eta * dW\n        \n    def update_b(self, eta, db):\n        self.b -= eta * db\n\nnp.set_printoptions(precision=8, suppress=False)\n\n# ----- Given values -----\n\n# Mapping for -1 and 1\ny = 2*y - 1\n\nHiddenLayers = []\n\nNLayers = 2\n\nfor _ in range(NLayers):\n    HiddenLayers.append(HiddenLayer(w_size=(2,2), b_size=(2,)))\n\n\nW2 = np.array([0.5, -0.3])                 # output layer weights (2,)\nb2 = 0.2                                   # output bias\n\neta = 0.1                                   # learning rate for the update step\ntanh = np.tanh\ntanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        prev_a = x_i\n        for layer in HiddenLayers:\n            layer.set_prev_a(prev_a)\n            layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)\n            a = (tanh(layer.z))                                # activations hidden (2,)\n            layer.set_a(a)\n            prev_a = a\n            \n            \n        z2 = W2 @ prev_a + b2           # pre-activation output\n        y_hat = tanh(z2)            # final output\n\n\n        L = ((y_i - y_hat)**2)\n\n        if L &gt; 1e-12:\n            \n            wrong = True\n\n            delta2 = (2.0 / N) * (y_hat - y_i) * (1.0 - y_hat**2)   # scalar\n\n            # Gradients for output layer\n            dW2 = delta2 * prev_a   \n            db2 = delta2\n\n            next_delta = delta2                      \n            next_W = W2                              \n\n            # Updatiung Hidden Layers\n            first_back = True\n            for layer in reversed(HiddenLayers):\n\n                if first_back:\n                    # Output Layer has a Bias (not a matrix)\n                    g = next_W * next_delta                   \n                    first_back = False\n                else:\n                    g = next_W.T @ next_delta                  \n\n                delta = g * (1.0 - layer.a**2)   \n\n                dW = np.outer(delta, layer.prev_a)     \n                db = delta                     \n\n                # Update hidden layer\n                layer.update_W(eta, dW)\n                layer.update_b(eta, db)\n\n                # Prepare for next layer\n                next_delta = delta\n                next_W = layer.W    \n\n            # Output Layer Update\n            W2 -= eta * dW2\n            b2 -= eta * db2\n                    \n    if not wrong:            \n        break\n\n\n\ndef forward(x, HiddenLayers, W2, b2):\n    x_ = x\n    for layer in HiddenLayers:\n        z = layer.W @ x_ + layer.b\n        a = np.tanh(z)\n        x_ = a\n    z2 = W2 @ x_ + b2\n    y_hat = np.tanh(z2)\n    return y_hat\n\n\n\ny_pred = np.array([forward(x_i, HiddenLayers, W2, b2) for x_i in X])\n\ny_pred_labels = np.where(y_pred &gt;= 0.0, 1, -1)\n\nprint(\"Final W2:\", W2, \"Final b2:\", b2)\nfor i, layer in enumerate(HiddenLayers, 1):\n    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n\nprint(\"Accuracy:\", accuracy_score(y, y_pred_labels))\n</pre> import numpy as np from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score   class HiddenLayer():          def __init__(self, w_size, b_size):         self.W = np.random.uniform(-1, 1, size=w_size)         self.b = np.random.uniform(-1, 1, size=b_size)         self.z = 0                # pre-activation value         self.a = 0                # activation value          self.prev_a = 0      def set_z(self, z):         self.z = z              def set_a(self, a):         self.a = a          def set_W(self, W):         self.W = W                  def set_b(self, b):         self.b = b              def set_prev_a(self, value):         self.prev_a = value                       def update_W(self, eta, dW):         self.W -= eta * dW              def update_b(self, eta, db):         self.b -= eta * db  np.set_printoptions(precision=8, suppress=False)  # ----- Given values -----  # Mapping for -1 and 1 y = 2*y - 1  HiddenLayers = []  NLayers = 2  for _ in range(NLayers):     HiddenLayers.append(HiddenLayer(w_size=(2,2), b_size=(2,)))   W2 = np.array([0.5, -0.3])                 # output layer weights (2,) b2 = 0.2                                   # output bias  eta = 0.1                                   # learning rate for the update step tanh = np.tanh tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          prev_a = x_i         for layer in HiddenLayers:             layer.set_prev_a(prev_a)             layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)             a = (tanh(layer.z))                                # activations hidden (2,)             layer.set_a(a)             prev_a = a                                   z2 = W2 @ prev_a + b2           # pre-activation output         y_hat = tanh(z2)            # final output           L = ((y_i - y_hat)**2)          if L &gt; 1e-12:                          wrong = True              delta2 = (2.0 / N) * (y_hat - y_i) * (1.0 - y_hat**2)   # scalar              # Gradients for output layer             dW2 = delta2 * prev_a                db2 = delta2              next_delta = delta2                                   next_W = W2                                            # Updatiung Hidden Layers             first_back = True             for layer in reversed(HiddenLayers):                  if first_back:                     # Output Layer has a Bias (not a matrix)                     g = next_W * next_delta                                        first_back = False                 else:                     g = next_W.T @ next_delta                                    delta = g * (1.0 - layer.a**2)                     dW = np.outer(delta, layer.prev_a)                      db = delta                                       # Update hidden layer                 layer.update_W(eta, dW)                 layer.update_b(eta, db)                  # Prepare for next layer                 next_delta = delta                 next_W = layer.W                  # Output Layer Update             W2 -= eta * dW2             b2 -= eta * db2                          if not wrong:                     break    def forward(x, HiddenLayers, W2, b2):     x_ = x     for layer in HiddenLayers:         z = layer.W @ x_ + layer.b         a = np.tanh(z)         x_ = a     z2 = W2 @ x_ + b2     y_hat = np.tanh(z2)     return y_hat    y_pred = np.array([forward(x_i, HiddenLayers, W2, b2) for x_i in X])  y_pred_labels = np.where(y_pred &gt;= 0.0, 1, -1)  print(\"Final W2:\", W2, \"Final b2:\", b2) for i, layer in enumerate(HiddenLayers, 1):     print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")  print(\"Accuracy:\", accuracy_score(y, y_pred_labels)) <pre>Final W2: [ 0.63446301 -0.83237545] Final b2: 0.45908843162023505\nLayer 1 weights:\n[[-0.3443021   0.88894019]\n [-0.09855254 -0.86185418]]\nLayer 1 bias:\n[-0.6156105   0.16952364]\nLayer 2 weights:\n[[ 0.72297534 -0.12167491]\n [-0.45737613  1.00072866]]\nLayer 2 bias:\n[0.72930515 0.43186679]\nAccuracy: 0.737\n</pre> In\u00a0[30]: Copied! <pre>import numpy as np\nfrom sklearn.datasets import make_classification\n\n# Reproducibility\nrng = np.random.RandomState(42)\n\ndef biased_weights_for(target_class, K=3, high=0.8):\n    \"\"\"\n    Return a weight vector of length K that heavily favors target_class.\n    Helps ensure enough samples of the desired class per call.\n    \"\"\"\n    low = (1.0 - high) / (K - 1)\n    w = np.full(K, low, dtype=float)\n    w[target_class] = high\n    return w\n\ndef sample_class_subset(\n    n_needed: int,\n    target_class: int,\n    n_clusters_per_class: int,\n    seed: int,\n    *,\n    n_features: int = 4,\n    n_informative: int = 4,\n    n_redundant: int = 0,\n    class_sep: float = 1.5,\n    flip_y: float = 0.0,\n    max_tries: int = 20,\n    K: int = 3\n):\n    \"\"\"\n    Generate samples with make_classification and keep only rows of 'target_class'.\n    We over-generate with biased 'weights' so we can downsample exactly n_needed.\n    \"\"\"\n    tries = 0\n    local_seed = seed\n    # Over-generate to boost the chance of hitting n_needed for target_class\n    n_generate = max(4 * n_needed, 2000)\n\n    while tries &lt; max_tries:\n        X_tmp, y_tmp = make_classification(\n            n_samples=n_generate,\n            n_features=n_features,\n            n_informative=n_informative,\n            n_redundant=n_redundant,\n            n_repeated=0,\n            n_classes=K,\n            n_clusters_per_class=n_clusters_per_class,\n            class_sep=class_sep,\n            flip_y=flip_y,\n            weights=biased_weights_for(target_class, K=K, high=0.8),\n            random_state=local_seed,\n        )\n\n        idx = np.flatnonzero(y_tmp == target_class)\n        if idx.size &gt;= n_needed:\n            chosen = rng.choice(idx, size=n_needed, replace=False)\n            return X_tmp[chosen], np.full(n_needed, target_class, dtype=int)\n\n        # Try again with a different seed\n        tries += 1\n        local_seed += 1\n\n    raise RuntimeError(\n        f\"Could not obtain {n_needed} samples for class={target_class} \"\n        f\"with n_clusters_per_class={n_clusters_per_class} after {max_tries} tries.\"\n    )\n\n# ---------- Build the asymmetric dataset ----------\nN = 1500\nK = 3\nn_per_class = [N // K] * K\nn_per_class[0] += N - sum(n_per_class)  # handle remainder if any (keeps total = N)\n\n# Assign distinct cluster counts per class\nclusters_per_class = {\n    0: 2,  # class 0 -&gt; 2 clusters\n    1: 3,  # class 1 -&gt; 3 clusters\n    2: 4,  # class 2 -&gt; 4 clusters\n}\n\n# Different seeds per class for variety\nbase_seeds = {0: 42, 1: 1337, 2: 2027}\n\nXs = []\nys = []\nfor c in range(K):\n    Xi, yi = sample_class_subset(\n        n_needed=n_per_class[c],\n        target_class=c,\n        n_clusters_per_class=clusters_per_class[c],\n        seed=base_seeds[c],\n        n_features=4,\n        n_informative=4,\n        n_redundant=0,\n        class_sep=1.6,   # tweak for difficulty vs. separability\n        flip_y=0.0,\n        K=K,\n    )\n    Xs.append(Xi)\n    ys.append(yi)\n\n# Combine and shuffle\nX = np.vstack(Xs)\ny = np.concatenate(ys)\nperm = rng.permutation(len(y))\nX = X[perm]\ny = y[perm]\n\nprint(\"X shape:\", X.shape, \"y shape:\", y.shape)\nprint(\"Class counts:\", np.bincount(y))\n# Expect: (1500, 4) and roughly balanced counts (exactly 500 each by construction)\n</pre> import numpy as np from sklearn.datasets import make_classification  # Reproducibility rng = np.random.RandomState(42)  def biased_weights_for(target_class, K=3, high=0.8):     \"\"\"     Return a weight vector of length K that heavily favors target_class.     Helps ensure enough samples of the desired class per call.     \"\"\"     low = (1.0 - high) / (K - 1)     w = np.full(K, low, dtype=float)     w[target_class] = high     return w  def sample_class_subset(     n_needed: int,     target_class: int,     n_clusters_per_class: int,     seed: int,     *,     n_features: int = 4,     n_informative: int = 4,     n_redundant: int = 0,     class_sep: float = 1.5,     flip_y: float = 0.0,     max_tries: int = 20,     K: int = 3 ):     \"\"\"     Generate samples with make_classification and keep only rows of 'target_class'.     We over-generate with biased 'weights' so we can downsample exactly n_needed.     \"\"\"     tries = 0     local_seed = seed     # Over-generate to boost the chance of hitting n_needed for target_class     n_generate = max(4 * n_needed, 2000)      while tries &lt; max_tries:         X_tmp, y_tmp = make_classification(             n_samples=n_generate,             n_features=n_features,             n_informative=n_informative,             n_redundant=n_redundant,             n_repeated=0,             n_classes=K,             n_clusters_per_class=n_clusters_per_class,             class_sep=class_sep,             flip_y=flip_y,             weights=biased_weights_for(target_class, K=K, high=0.8),             random_state=local_seed,         )          idx = np.flatnonzero(y_tmp == target_class)         if idx.size &gt;= n_needed:             chosen = rng.choice(idx, size=n_needed, replace=False)             return X_tmp[chosen], np.full(n_needed, target_class, dtype=int)          # Try again with a different seed         tries += 1         local_seed += 1      raise RuntimeError(         f\"Could not obtain {n_needed} samples for class={target_class} \"         f\"with n_clusters_per_class={n_clusters_per_class} after {max_tries} tries.\"     )  # ---------- Build the asymmetric dataset ---------- N = 1500 K = 3 n_per_class = [N // K] * K n_per_class[0] += N - sum(n_per_class)  # handle remainder if any (keeps total = N)  # Assign distinct cluster counts per class clusters_per_class = {     0: 2,  # class 0 -&gt; 2 clusters     1: 3,  # class 1 -&gt; 3 clusters     2: 4,  # class 2 -&gt; 4 clusters }  # Different seeds per class for variety base_seeds = {0: 42, 1: 1337, 2: 2027}  Xs = [] ys = [] for c in range(K):     Xi, yi = sample_class_subset(         n_needed=n_per_class[c],         target_class=c,         n_clusters_per_class=clusters_per_class[c],         seed=base_seeds[c],         n_features=4,         n_informative=4,         n_redundant=0,         class_sep=1.6,   # tweak for difficulty vs. separability         flip_y=0.0,         K=K,     )     Xs.append(Xi)     ys.append(yi)  # Combine and shuffle X = np.vstack(Xs) y = np.concatenate(ys) perm = rng.permutation(len(y)) X = X[perm] y = y[perm]  print(\"X shape:\", X.shape, \"y shape:\", y.shape) print(\"Class counts:\", np.bincount(y)) # Expect: (1500, 4) and roughly balanced counts (exactly 500 each by construction)  <pre>X shape: (1500, 4) y shape: (1500,)\nClass counts: [500 500 500]\n</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n\ndef softmax(z):\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef one_hot(y_i, K):\n    v = np.zeros(K, dtype=float)\n    v[y_i] = 1.0\n    return v\n\n\n\n\nclass HiddenLayer():\n    \n    def __init__(self, w_size, b_size):\n        self.W = np.random.uniform(-1, 1, size=w_size)\n        self.b = np.random.uniform(-1, 1, size=b_size)\n        self.z = 0                # pre-activation value\n        self.a = 0                # activation value \n        self.prev_a = 0\n\n    def set_z(self, z):\n        self.z = z\n        \n    def set_a(self, a):\n        self.a = a\n    \n    def set_W(self, W):\n        self.W = W    \n        \n    def set_b(self, b):\n        self.b = b\n        \n    def set_prev_a(self, value):\n        self.prev_a = value\n        \n        \n    def update_W(self, eta, dW):\n        self.W -= eta * dW\n        \n    def update_b(self, eta, db):\n        self.b -= eta * db\n\n\n\n\n\n\ndef forward_probs(x, HiddenLayers, W2, b2):\n    prev_a = x\n    for layer in HiddenLayers:\n        layer.set_prev_a(prev_a)\n        layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)\n        a = (tanh(layer.z))                                # activations hidden (2,)\n        layer.set_a(a)\n        prev_a = a\n    z2 = W2 @ prev_a + b2          # logits, shape (K,)\n    p = softmax(z2)           # probs, shape (K,)\n    return p, prev_a               # also return last hidden activation\n\n\n\n\nK = len(np.unique(y))                 # number of classes\ninput_dim = X.shape[1]                # = 4\nH = 3                                 # hidden width (it is up to personal prefeerence)\nNLayers = 2\n\nHiddenLayers = []\n# First hidden layer: (H, input_dim)\nHiddenLayers.append(HiddenLayer(w_size=(H, input_dim), b_size=(H,)))\n\n# Remaining hidden layers: (H, H)\nfor _ in range(NLayers - 1):\n    HiddenLayers.append(HiddenLayer(w_size=(H, H), b_size=(H,)))\n\n# Output layer: (K, H) and (K,)\nW2 = np.random.uniform(-1, 1, size=(K, H))\nb2 = np.zeros(K)                \n\n\n\neta = 0.1                                   # learning rate for the update step\ntanh = np.tanh\ntanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)  # p: (K,), a_last: (H,)\n        L = -np.log(p[y_i])   \n\n        if L &gt; 1e-12:\n            \n            wrong = True\n\n            # output layer calculations\n            y_one = one_hot(y_i, K)               # (K,)\n            delta_out = p - y_one                 # (K,)\n            dW2 = np.outer(delta_out, a_last)     # (K, H)\n            db2 = delta_out                       # (K,)\n\n            # save for hidden backprop (use current W2, not yet updated)\n            next_delta = delta_out                # (K,)\n            next_W = W2                           # (K, H)\n\n            # updating hidden layers from reversed order\n            for layer in reversed(HiddenLayers):\n                g = next_W.T @ next_delta         # (H_prev,) where H_prev = layer.a.size\n                delta = g * (1.0 - layer.a**2)    # tanh'(z) = 1 - a^2\n\n                dW = np.outer(delta, layer.prev_a)\n                db = delta\n\n                layer.update_W(eta, dW)\n                layer.update_b(eta, db)\n\n                next_delta = delta                # (H_prev,)\n                next_W = layer.W                  # (H_prev, input_dim_prev)\n\n            # Updatye Output Layer\n            W2 -= eta * dW2\n            b2 -= eta * db2\n                    \n    if not wrong:            \n        break\n\n\n\ndef forward(x, HiddenLayers, W2, b2):\n    x_ = x\n    for layer in HiddenLayers:\n        z = layer.W @ x_ + layer.b\n        a = np.tanh(z)\n        x_ = a\n    z2 = W2 @ x_ + b2\n    y_hat = np.tanh(z2)\n    return y_hat\n\n\n\nprint(\"Final W2:\", W2, \"Final b2:\", b2)\nfor i, layer in enumerate(HiddenLayers, 1):\n    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n\nprobs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X])  # (N, K)\ny_pred = np.argmax(probs, axis=1)\nprint(\"Accuracy:\", accuracy_score(y, y_pred))\n</pre> import numpy as np from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score   def softmax(z):     z = z - np.max(z)     e = np.exp(z)     return e / np.sum(e)  def one_hot(y_i, K):     v = np.zeros(K, dtype=float)     v[y_i] = 1.0     return v     class HiddenLayer():          def __init__(self, w_size, b_size):         self.W = np.random.uniform(-1, 1, size=w_size)         self.b = np.random.uniform(-1, 1, size=b_size)         self.z = 0                # pre-activation value         self.a = 0                # activation value          self.prev_a = 0      def set_z(self, z):         self.z = z              def set_a(self, a):         self.a = a          def set_W(self, W):         self.W = W                  def set_b(self, b):         self.b = b              def set_prev_a(self, value):         self.prev_a = value                       def update_W(self, eta, dW):         self.W -= eta * dW              def update_b(self, eta, db):         self.b -= eta * db       def forward_probs(x, HiddenLayers, W2, b2):     prev_a = x     for layer in HiddenLayers:         layer.set_prev_a(prev_a)         layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)         a = (tanh(layer.z))                                # activations hidden (2,)         layer.set_a(a)         prev_a = a     z2 = W2 @ prev_a + b2          # logits, shape (K,)     p = softmax(z2)           # probs, shape (K,)     return p, prev_a               # also return last hidden activation     K = len(np.unique(y))                 # number of classes input_dim = X.shape[1]                # = 4 H = 3                                 # hidden width (it is up to personal prefeerence) NLayers = 2  HiddenLayers = [] # First hidden layer: (H, input_dim) HiddenLayers.append(HiddenLayer(w_size=(H, input_dim), b_size=(H,)))  # Remaining hidden layers: (H, H) for _ in range(NLayers - 1):     HiddenLayers.append(HiddenLayer(w_size=(H, H), b_size=(H,)))  # Output layer: (K, H) and (K,) W2 = np.random.uniform(-1, 1, size=(K, H)) b2 = np.zeros(K)                    eta = 0.1                                   # learning rate for the update step tanh = np.tanh tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)  # p: (K,), a_last: (H,)         L = -np.log(p[y_i])             if L &gt; 1e-12:                          wrong = True              # output layer calculations             y_one = one_hot(y_i, K)               # (K,)             delta_out = p - y_one                 # (K,)             dW2 = np.outer(delta_out, a_last)     # (K, H)             db2 = delta_out                       # (K,)              # save for hidden backprop (use current W2, not yet updated)             next_delta = delta_out                # (K,)             next_W = W2                           # (K, H)              # updating hidden layers from reversed order             for layer in reversed(HiddenLayers):                 g = next_W.T @ next_delta         # (H_prev,) where H_prev = layer.a.size                 delta = g * (1.0 - layer.a**2)    # tanh'(z) = 1 - a^2                  dW = np.outer(delta, layer.prev_a)                 db = delta                  layer.update_W(eta, dW)                 layer.update_b(eta, db)                  next_delta = delta                # (H_prev,)                 next_W = layer.W                  # (H_prev, input_dim_prev)              # Updatye Output Layer             W2 -= eta * dW2             b2 -= eta * db2                          if not wrong:                     break    def forward(x, HiddenLayers, W2, b2):     x_ = x     for layer in HiddenLayers:         z = layer.W @ x_ + layer.b         a = np.tanh(z)         x_ = a     z2 = W2 @ x_ + b2     y_hat = np.tanh(z2)     return y_hat    print(\"Final W2:\", W2, \"Final b2:\", b2) for i, layer in enumerate(HiddenLayers, 1):     print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")  probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X])  # (N, K) y_pred = np.argmax(probs, axis=1) print(\"Accuracy:\", accuracy_score(y, y_pred))  <pre>Final W2: [[-1.30150104  0.74545721  0.40928816]\n [ 1.46128138  0.86141196 -0.66732008]\n [ 1.12006045 -1.26491417  1.06276974]] Final b2: [-1.23020757 -0.18742653  1.4176341 ]\nLayer 1 weights:\n[[ 14.87125433   4.96105463  -6.61067946   9.02410079]\n [ -1.54703905   0.30016101  19.98290756   3.37650402]\n [  6.28185814 -12.02192067   8.7366937   -3.54556444]]\nLayer 1 bias:\n[-0.44119326 -6.82410816 -7.93830413]\nLayer 2 weights:\n[[-0.75370642  2.52967955  1.27164864]\n [-4.13644548 -2.32339937 -1.9046856 ]\n [-1.02515081 -1.44959544  2.82048605]]\nLayer 2 bias:\n[-0.94398869  2.25227241  1.17658551]\nAccuracy: 0.7086666666666667\n</pre> In\u00a0[35]: Copied! <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n\ndef softmax(z):\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef one_hot(y_i, K):\n    v = np.zeros(K, dtype=float)\n    v[y_i] = 1.0\n    return v\n\n\n\n\nclass HiddenLayer():\n    \n    def __init__(self, w_size, b_size):\n        self.W = np.random.uniform(-1, 1, size=w_size)\n        self.b = np.random.uniform(-1, 1, size=b_size)\n        self.z = 0                # pre-activation value\n        self.a = 0                # activation value \n        self.prev_a = 0\n\n    def set_z(self, z):\n        self.z = z\n        \n    def set_a(self, a):\n        self.a = a\n    \n    def set_W(self, W):\n        self.W = W    \n        \n    def set_b(self, b):\n        self.b = b\n        \n    def set_prev_a(self, value):\n        self.prev_a = value\n        \n        \n    def update_W(self, eta, dW):\n        self.W -= eta * dW\n        \n    def update_b(self, eta, db):\n        self.b -= eta * db\n\n\n\n\n\n\ndef forward_probs(x, HiddenLayers, W2, b2):\n    prev_a = x\n    for layer in HiddenLayers:\n        layer.set_prev_a(prev_a)\n        layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)\n        a = (tanh(layer.z))                                # activations hidden (2,)\n        layer.set_a(a)\n        prev_a = a\n    z2 = W2 @ prev_a + b2          # logits, shape (K,)\n    p = softmax(z2)           # probs, shape (K,)\n    return p, prev_a               # also return last hidden activation\n\n\n\n\nK = len(np.unique(y))                 # number of classes\ninput_dim = X.shape[1]                # = 4\nH = 3                                 # hidden width (it is up to personal prefeerence)\nNLayers = 28\n\nHiddenLayers = []\n# First hidden layer: (H, input_dim)\nHiddenLayers.append(HiddenLayer(w_size=(H, input_dim), b_size=(H,)))\n\n# Remaining hidden layers: (H, H)\nfor _ in range(NLayers - 1):\n    HiddenLayers.append(HiddenLayer(w_size=(H, H), b_size=(H,)))\n\n# Output layer: (K, H) and (K,)\nW2 = np.random.uniform(-1, 1, size=(K, H))\nb2 = np.zeros(K)                \n\n\n\neta = 0.1                                   # learning rate for the update step\ntanh = np.tanh\ntanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)  # p: (K,), a_last: (H,)\n        L = -np.log(p[y_i])   \n\n        if L &gt; 1e-12:\n            \n            wrong = True\n\n            # output layer calculations\n            y_one = one_hot(y_i, K)               # (K,)\n            delta_out = p - y_one                 # (K,)\n            dW2 = np.outer(delta_out, a_last)     # (K, H)\n            db2 = delta_out                       # (K,)\n\n            # save for hidden backprop (use current W2, not yet updated)\n            next_delta = delta_out                # (K,)\n            next_W = W2                           # (K, H)\n\n            # updating hidden layers from reversed order\n            for layer in reversed(HiddenLayers):\n                g = next_W.T @ next_delta         # (H_prev,) where H_prev = layer.a.size\n                delta = g * (1.0 - layer.a**2)    # tanh'(z) = 1 - a^2\n\n                dW = np.outer(delta, layer.prev_a)\n                db = delta\n\n                layer.update_W(eta, dW)\n                layer.update_b(eta, db)\n\n                next_delta = delta                # (H_prev,)\n                next_W = layer.W                  # (H_prev, input_dim_prev)\n\n            # Updatye Output Layer\n            W2 -= eta * dW2\n            b2 -= eta * db2\n                    \n    if not wrong:            \n        break\n\n\n\ndef forward(x, HiddenLayers, W2, b2):\n    x_ = x\n    for layer in HiddenLayers:\n        z = layer.W @ x_ + layer.b\n        a = np.tanh(z)\n        x_ = a\n    z2 = W2 @ x_ + b2\n    y_hat = np.tanh(z2)\n    return y_hat\n\n\n\nprint(\"Final W2:\", W2, \"Final b2:\", b2)\nfor i, layer in enumerate(HiddenLayers, 1):\n    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n\nprobs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X])  # (N, K)\ny_pred = np.argmax(probs, axis=1)\nprint(\"Accuracy:\", accuracy_score(y, y_pred))\n</pre> import numpy as np from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score   def softmax(z):     z = z - np.max(z)     e = np.exp(z)     return e / np.sum(e)  def one_hot(y_i, K):     v = np.zeros(K, dtype=float)     v[y_i] = 1.0     return v     class HiddenLayer():          def __init__(self, w_size, b_size):         self.W = np.random.uniform(-1, 1, size=w_size)         self.b = np.random.uniform(-1, 1, size=b_size)         self.z = 0                # pre-activation value         self.a = 0                # activation value          self.prev_a = 0      def set_z(self, z):         self.z = z              def set_a(self, a):         self.a = a          def set_W(self, W):         self.W = W                  def set_b(self, b):         self.b = b              def set_prev_a(self, value):         self.prev_a = value                       def update_W(self, eta, dW):         self.W -= eta * dW              def update_b(self, eta, db):         self.b -= eta * db       def forward_probs(x, HiddenLayers, W2, b2):     prev_a = x     for layer in HiddenLayers:         layer.set_prev_a(prev_a)         layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)         a = (tanh(layer.z))                                # activations hidden (2,)         layer.set_a(a)         prev_a = a     z2 = W2 @ prev_a + b2          # logits, shape (K,)     p = softmax(z2)           # probs, shape (K,)     return p, prev_a               # also return last hidden activation     K = len(np.unique(y))                 # number of classes input_dim = X.shape[1]                # = 4 H = 3                                 # hidden width (it is up to personal prefeerence) NLayers = 28  HiddenLayers = [] # First hidden layer: (H, input_dim) HiddenLayers.append(HiddenLayer(w_size=(H, input_dim), b_size=(H,)))  # Remaining hidden layers: (H, H) for _ in range(NLayers - 1):     HiddenLayers.append(HiddenLayer(w_size=(H, H), b_size=(H,)))  # Output layer: (K, H) and (K,) W2 = np.random.uniform(-1, 1, size=(K, H)) b2 = np.zeros(K)                    eta = 0.1                                   # learning rate for the update step tanh = np.tanh tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)  # p: (K,), a_last: (H,)         L = -np.log(p[y_i])             if L &gt; 1e-12:                          wrong = True              # output layer calculations             y_one = one_hot(y_i, K)               # (K,)             delta_out = p - y_one                 # (K,)             dW2 = np.outer(delta_out, a_last)     # (K, H)             db2 = delta_out                       # (K,)              # save for hidden backprop (use current W2, not yet updated)             next_delta = delta_out                # (K,)             next_W = W2                           # (K, H)              # updating hidden layers from reversed order             for layer in reversed(HiddenLayers):                 g = next_W.T @ next_delta         # (H_prev,) where H_prev = layer.a.size                 delta = g * (1.0 - layer.a**2)    # tanh'(z) = 1 - a^2                  dW = np.outer(delta, layer.prev_a)                 db = delta                  layer.update_W(eta, dW)                 layer.update_b(eta, db)                  next_delta = delta                # (H_prev,)                 next_W = layer.W                  # (H_prev, input_dim_prev)              # Updatye Output Layer             W2 -= eta * dW2             b2 -= eta * db2                          if not wrong:                     break    def forward(x, HiddenLayers, W2, b2):     x_ = x     for layer in HiddenLayers:         z = layer.W @ x_ + layer.b         a = np.tanh(z)         x_ = a     z2 = W2 @ x_ + b2     y_hat = np.tanh(z2)     return y_hat    print(\"Final W2:\", W2, \"Final b2:\", b2) for i, layer in enumerate(HiddenLayers, 1):     print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")  probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X])  # (N, K) y_pred = np.argmax(probs, axis=1) print(\"Accuracy:\", accuracy_score(y, y_pred))  <pre>Final W2: [[-0.84814546  0.53060569 -0.02411378]\n [-0.84814546  0.53060569 -0.02411379]\n [-0.84814546  0.53060569 -0.02411378]] Final b2: [-3.47440006e-05 -2.60865489e-01  2.60900233e-01]\nLayer 1 weights:\n[[-0.84338989  0.61003512  0.19936643  0.24624894]\n [-0.3643678  -0.15606804  0.51052957 -0.82817205]\n [ 0.78685002 -0.21631564  0.33439671  0.32546589]]\nLayer 1 bias:\n[0.71062903 0.57992    0.99769994]\nLayer 2 weights:\n[[ 0.92091437  0.1590592  -0.92093574]\n [ 0.26167634 -0.37245274 -0.51969802]\n [ 0.52611436  0.27042889  0.56091939]]\nLayer 2 bias:\n[-0.94674719 -0.50250292 -0.88996456]\nLayer 3 weights:\n[[-0.48557048 -0.89524249  0.23999672]\n [ 0.11486781  0.22516431  0.4767062 ]\n [ 0.72083311  0.25566272 -0.04362071]]\nLayer 3 bias:\n[-0.87372412  0.31243484 -0.1821496 ]\nLayer 4 weights:\n[[-0.86679364 -0.74903253 -0.59972829]\n [-0.41471671 -0.93718506  0.99308781]\n [-0.90901745 -0.38878322 -0.66119217]]\nLayer 4 bias:\n[ 0.84725279 -0.47609917 -0.78251266]\nLayer 5 weights:\n[[ 0.9030442   0.58482892 -0.43559953]\n [-0.41552525 -0.21690784  0.5429586 ]\n [-0.49544896  0.30894673 -0.67021008]]\nLayer 5 bias:\n[-0.65896629 -0.61186907  0.5217721 ]\nLayer 6 weights:\n[[-0.55519639 -0.21116559  0.92635114]\n [-0.8295121   0.74982627 -0.5204944 ]\n [ 0.58045514 -0.11925096  0.91129031]]\nLayer 6 bias:\n[ 0.63632091  0.69050396 -0.34407802]\nLayer 7 weights:\n[[ 0.84706671 -0.74466787 -0.93739272]\n [ 0.06601499 -0.76135376  0.19264094]\n [ 0.66664853  0.971907   -0.12379642]]\nLayer 7 bias:\n[0.6458024  0.0991511  0.56741277]\nLayer 8 weights:\n[[-0.49792103 -0.33986334  0.64831763]\n [-0.67284879 -0.0911487  -0.43292653]\n [ 0.69657043 -0.92378961 -0.46666754]]\nLayer 8 bias:\n[0.96021288 0.52926684 0.43696608]\nLayer 9 weights:\n[[-0.02879622 -0.09095586  0.10260813]\n [-0.18308815  0.88207993 -0.81538387]\n [ 0.07355429  0.90624164  0.44765395]]\nLayer 9 bias:\n[-0.61947583  0.41456654  0.15003394]\nLayer 10 weights:\n[[-0.38670175  0.37181863  0.74573708]\n [-0.58474177  0.17708032 -0.05597755]\n [-0.1046401   0.18846164 -0.62779381]]\nLayer 10 bias:\n[ 0.778389   -0.11774065 -0.58456906]\nLayer 11 weights:\n[[ 0.29578964  0.70808643 -0.71902162]\n [-0.2780659  -0.15756649 -0.52731511]\n [-0.81969092 -0.99186894 -0.27165441]]\nLayer 11 bias:\n[ 0.21351485 -0.50396314 -0.60744244]\nLayer 12 weights:\n[[ 0.20899755  0.188431    0.9039075 ]\n [ 0.73453118  0.72469553  0.67449315]\n [-0.43761628  0.50199741 -0.00176604]]\nLayer 12 bias:\n[-0.47384312 -0.41409483  0.34247563]\nLayer 13 weights:\n[[ 0.85827974  0.26050644  0.99279375]\n [ 0.26630617  0.42527993 -0.98437894]\n [ 0.72432677  0.44440174 -0.07706986]]\nLayer 13 bias:\n[ 0.81259266 -0.65490135 -0.23438245]\nLayer 14 weights:\n[[-0.32600582 -0.14099059  0.7022149 ]\n [ 0.30027051  0.54585321  0.67737026]\n [-0.60452063  0.25479585 -0.1465644 ]]\nLayer 14 bias:\n[ 0.09187532 -0.93153263  0.82949807]\nLayer 15 weights:\n[[-0.07802049  0.78518034  0.81796399]\n [-0.38031279 -0.9721498   0.93982652]\n [-0.26769681 -0.99869141  0.82723497]]\nLayer 15 bias:\n[ 0.96983261 -0.19004694 -0.27436936]\nLayer 16 weights:\n[[ 0.56836158 -0.63261343  0.20244483]\n [-0.90876047  0.51555323 -0.60564177]\n [ 0.0925004   0.79601933 -0.84226849]]\nLayer 16 bias:\n[-0.39219228 -0.88461593  0.64174938]\nLayer 17 weights:\n[[-0.04223624 -0.0633029  -0.89187892]\n [ 0.47756719  0.55116613 -0.64863496]\n [ 0.24527797  0.18346612  0.69450946]]\nLayer 17 bias:\n[-0.55650148 -0.99207706 -0.06485824]\nLayer 18 weights:\n[[-0.39748889  0.16881872 -0.08851347]\n [ 0.35963021  0.70508191  0.74982946]\n [ 0.22187427 -0.37280188 -0.6715373 ]]\nLayer 18 bias:\n[ 0.07629452 -0.09305782  0.08393797]\nLayer 19 weights:\n[[ 0.07315602  0.16119645  0.06284187]\n [-0.85287028  0.70676101 -0.38580857]\n [ 0.20085256  0.97611168 -0.04219756]]\nLayer 19 bias:\n[ 0.83181074  0.85390894 -0.97626914]\nLayer 20 weights:\n[[ 0.09332611  0.54858288 -0.34970708]\n [-0.19122545  0.9336954  -0.9673755 ]\n [ 0.25725625  0.661262    0.17482299]]\nLayer 20 bias:\n[-0.93241732 -0.81781579 -0.22165913]\nLayer 21 weights:\n[[ 0.76171395 -0.09259759  0.02512992]\n [-0.77192088 -0.31653661  0.89888902]\n [-0.19789061  0.23023854 -0.79608376]]\nLayer 21 bias:\n[-0.37668546  0.84282715  0.72127689]\nLayer 22 weights:\n[[-0.63804925  0.34138177 -0.63317894]\n [-0.20310024 -0.93926575 -0.7620111 ]\n [ 0.30841294  0.97155075  0.24961694]]\nLayer 22 bias:\n[ 0.57131956 -0.48648027  0.63087965]\nLayer 23 weights:\n[[ 0.11101438  0.11443556 -0.62925282]\n [-0.83833369  0.81172015  0.37005955]\n [-0.68288991  0.75208239  0.47909997]]\nLayer 23 bias:\n[-0.22936561  0.34149771 -0.15579547]\nLayer 24 weights:\n[[ 0.39669916 -0.73288457 -0.21538437]\n [ 0.15164198 -0.24233378 -0.7001719 ]\n [ 0.92853424  0.82661268  0.96074571]]\nLayer 24 bias:\n[0.28035122 0.42092809 0.83142552]\nLayer 25 weights:\n[[ 0.09671779 -0.12517287  0.1166779 ]\n [ 0.3213752   0.40228999  0.33406393]\n [ 0.37184415 -0.77764378  0.12140295]]\nLayer 25 bias:\n[-0.94781249  0.87108809 -0.81741833]\nLayer 26 weights:\n[[-0.47884978  0.80867635  0.04036849]\n [-1.11134796  0.59463244  0.36112353]\n [-0.67632998 -0.56671283  0.49569751]]\nLayer 26 bias:\n[ 0.14444695  0.10312196 -0.83345851]\nLayer 27 weights:\n[[ 0.69581258  0.50586944 -0.96834487]\n [-1.07041915 -0.35764253 -0.61594017]\n [-0.21659356  0.79271223  0.74534188]]\nLayer 27 bias:\n[0.06538256 0.36249442 0.603774  ]\nLayer 28 weights:\n[[ 0.75887268 -0.25837588 -0.7857883 ]\n [ 0.31733201  0.65501039  0.09359379]\n [-0.64907728  0.07766857  0.45316614]]\nLayer 28 bias:\n[-0.45701711 -0.15716434  0.44452645]\nAccuracy: 0.3333333333333333\n</pre>"},{"location":"Exercicios/MLP/ex3_MLP/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1: Manual Calculation of MLP Steps\u00b6","text":"<p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): $ L = \\frac{1}{N} (y - \\hat{y})^2 $, where $ \\hat{y} $ is the network's output.</p> <p>For this exercise, use the following specific values:</p> <ul> <li><p>Input and output vectors:</p> <p>$ \\mathbf{x} = [0.5, -0.2] $</p> <p>$ y = 1.0 $</p> </li> <li><p>Hidden layer weights:</p> <p>$ \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} $</p> </li> <li><p>Hidden layer biases:</p> <p>$ \\mathbf{b}^{(1)} = [0.1, -0.2] $</p> </li> <li><p>Output layer weights:</p> <p>$ \\mathbf{W}^{(2)} = [0.5, -0.3] $</p> </li> <li><p>Output layer bias:</p> <p>$ b^{(2)} = 0.2 $</p> </li> <li><p>Learning rate: $ \\eta = 0.3 $</p> </li> <li><p>Activation function: $ \\tanh $</p> </li> </ul> <p>Values were defined as follows :</p> <pre>x  = np.array([0.5, -0.2])                 # input\ny  = 1.0                                   # target\n\n\nW1 = np.array([[0.3, -0.1],                # hidden layer weights (2x2)\n               [0.2,  0.4]])\n\n\nb1 = np.array([0.1, -0.2])                 # hidden biases\n\nW2 = np.array([0.5, -0.3])                 # output layer weights\nb2 = 0.2                                   # output bias\n\neta = 0.1                                  # learning rate, used for the update step\ntanh = np.tanh\ntanhp = lambda z: 1.0 - np.tanh(z)**2      # derivative of tanh\n</pre> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p> <ol> <li><p>Forward Pass:</p> <ul> <li>Compute the hidden layer pre-activations: $ \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} $.</li> <li>Apply tanh to get hidden activations: $ \\mathbf{a}^{(1)} = \\tanh(\\mathbf{z}^{(1)}) $.</li> <li>Compute the output pre-activation: $ z^{(2)} = \\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + b^{(2)} $.</li> <li>Compute the final output: $ \\hat{y} = \\tanh(z^{(2)}) $.</li> </ul> </li> </ol> <pre># ----- 1) Forward pass -----\nz1 = W1 @ x + b1            # hidden-layer pre-activations\na1 = tanh(z1)               # hidden activations (2,)\nz2 = W2 @ a1 + b2           # pre-activation output \ny_hat = tanh(z2)            # final output\n</pre> <ol> <li><p>Loss Calculation:</p> <ul> <li><p>Compute the MSE loss:</p> <p>$ L = \\frac{1}{N} (y - \\hat{y})^2 $.</p> </li> </ul> </li> </ol> <pre># ----- 2) Loss (MSE, N=1) -----\nL = (y - y_hat)**2\n</pre> <ol> <li><p>Backward Pass (Backpropagation): Compute the gradients of the loss with respect to all weights and biases. Start with $ \\frac{\\partial L}{\\partial \\hat{y}} $, then compute:</p> <ul> <li>$ \\frac{\\partial L}{\\partial z^{(2)}} $ (using the tanh derivative: $ \\frac{d}{dz} \\tanh(z) = 1 - \\tanh^2(z) $).</li> <li>Gradients for output layer: $ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} $, $ \\frac{\\partial L}{\\partial b^{(2)}} $.</li> <li>Propagate to hidden layer: $ \\frac{\\partial L}{\\partial \\mathbf{a}^{(1)}} $, $ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} $.</li> <li>Gradients for hidden layer: $ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} $, $ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} $.</li> </ul> <p>Show all intermediate steps and calculations.</p> </li> </ol> <pre># ----- 3) Backward pass -----\n# dL/dy_hat\ndL_dyhat = 2.0 * (y_hat - y)              # since N=1\n# dL/dz2\ndL_dz2 = dL_dyhat * tanhp(z2)\n\n# Output layer grads\n# dL/dW2 = dL/dz2 * a1\ndL_dW2 = dL_dz2 * a1\n# dL/db2 = dL/dz2\ndL_db2 = dL_dz2\n\n# Backprop to hidden\n# dL/da1 = dL/dz2 * W2\ndL_da1 = dL_dz2 * W2\n# dL/dz1 = dL/da1 \u2299 tanh'(z1)\ndL_dz1 = dL_da1 * tanhp(z1)\n\n# Hidden layer grads\n# dL/dW1 = (dL/dz1)[:, None] @ x[None, :]\ndL_dW1 = np.outer(dL_dz1, x)\n# dL/db1 = dL/dz1\ndL_db1 = dL_dz1\n</pre> <ol> <li><p>Parameter Update: Using the learning rate $ \\eta = 0.1 $, update all weights and biases via gradient descent:</p> <ul> <li>$ \\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} $</li> <li>$ b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} $</li> <li>$ \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} $</li> <li>$ \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} $</li> </ul> <p>Provide the numerical values for all updated parameters.</p> </li> </ol> <pre># ----- 4) Parameter update (gradient descent, eta = 0.1) -----\n\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n</pre> <p>Submission Requirements: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p>"},{"location":"Exercicios/MLP/ex3_MLP/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2: Binary Classification with Synthetic Data and Scratch MLP\u00b6","text":"<p>Using the <code>make_classification</code> function from scikit-learn (documentation), generate a synthetic dataset with the following specifications:</p> <ul> <li>Number of samples: 1000</li> <li>Number of classes: 2</li> <li>Number of clusters per class: Use the <code>n_clusters_per_class</code> parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</li> <li>Other parameters: Set <code>n_features=2</code> for easy visualization, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> for reproducibility, and adjust <code>class_sep</code> or <code>flip_y</code> as needed for a challenging but separable dataset.</li> </ul> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li>Number of hidden layers (at least 1)</li> <li>Number of neurons per layer</li> <li>Activation functions (e.g., sigmoid, ReLU, tanh)</li> <li>Loss function (e.g., binary cross-entropy)</li> <li>Optimizer (e.g., gradient descent, with a chosen learning rate)</li> </ul> <p>Steps to follow:</p> <ol> <li>Generate and split the data into training (80%) and testing (20%) sets.</li> <li>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</li> <li>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</li> <li>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</li> <li>Submit your code and results, including any visualizations.</li> </ol>"},{"location":"Exercicios/MLP/ex3_MLP/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp","title":"Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP\u00b6","text":"<p>Similar to Exercise 2, but with increased complexity.</p> <p>Use <code>make_classification</code> to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ol> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ol>"},{"location":"Exercicios/MLP/ex3_MLP/#exercise-4-multi-class-classification-with-deeper-mlp","title":"Exercise 4: Multi-Class Classification with Deeper MLP\u00b6","text":"<p>Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/","title":"Exercicio 2 - Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nmu_A = [2, 2]\nSigma_A = np.array([\n    [0.5, 0.0],\n    [0.0, 0.5]\n])\n\nmu_B = [5, 5]\nSigma_B = np.array([\n    [0.5, 0.0],\n    [0.0, 0.5]\n])\n\nclass_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nclass_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack((class_A, class_B))\ny = np.array([-1]*500 + [1]*500)\n\nprint(\"Dataset shape:\", X.shape)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA  np.random.seed(42)  mu_A = [2, 2] Sigma_A = np.array([     [0.5, 0.0],     [0.0, 0.5] ])  mu_B = [5, 5] Sigma_B = np.array([     [0.5, 0.0],     [0.0, 0.5] ])  class_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500) class_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack((class_A, class_B)) y = np.array([-1]*500 + [1]*500)  print(\"Dataset shape:\", X.shape)  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") plt.legend() plt.show()  <pre>Dataset shape: (1000, 2)\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import accuracy_score\n\nw = np.zeros(2, dtype=float)\nb = 0\nalpha = 0.01\n\ndef activation(scores):\n    return np.where(scores &gt;= 0, 1, -1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        score = np.dot(w, x_i) + b\n        \n        y_pred = activation(score)\n        \n        error = y_i - y_pred\n        \n        if error != 0:\n            w += alpha * error * x_i \n            b += alpha * error\n            wrong = True\n\n    if not wrong:            \n        break\n\nscores = X @ w + b\ny_pred = activation(scores)\n\nprint(\"Final weights:\", w, \"bias:\", b)\nprint(\"Accuracy:\", accuracy_score(y, y_pred))\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nx_plot = np.linspace(0, 10, 100)\ny_plot = -(w[0]/w[1])*x_plot - b/w[1]\nplt.plot(x_plot, y_plot, 'r-')\nplt.legend()\nplt.show()\n</pre> import numpy as np from sklearn.metrics import accuracy_score  w = np.zeros(2, dtype=float) b = 0 alpha = 0.01  def activation(scores):     return np.where(scores &gt;= 0, 1, -1)  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          score = np.dot(w, x_i) + b                  y_pred = activation(score)                  error = y_i - y_pred                  if error != 0:             w += alpha * error * x_i              b += alpha * error             wrong = True      if not wrong:                     break  scores = X @ w + b y_pred = activation(scores)  print(\"Final weights:\", w, \"bias:\", b) print(\"Accuracy:\", accuracy_score(y, y_pred))  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") x_plot = np.linspace(0, 10, 100) y_plot = -(w[0]/w[1])*x_plot - b/w[1] plt.plot(x_plot, y_plot, 'r-') plt.legend() plt.show() <pre>Final weights: [0.34065515 0.15323602] bias: -1.4200000000000008\nAccuracy: 0.975\n</pre> <p>The data is well separated, meaning we can divide the two classes with a straight line. Fixing the weights and bias doesn't take long because of the nature of the data</p> In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nmu_A = [3, 3]\nSigma_A = np.array([\n    [1.5, 0.0],\n    [0.0, 1.5]\n])\n\nmu_B = [4, 4]\nSigma_B = np.array([\n    [1.5, 0.0],\n    [0.0, 1.5]\n])\n\nclass_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nclass_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack((class_A, class_B))\ny = np.array([-1]*500 + [1]*500)\n\nprint(\"Dataset shape:\", X.shape)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA  np.random.seed(42)  mu_A = [3, 3] Sigma_A = np.array([     [1.5, 0.0],     [0.0, 1.5] ])  mu_B = [4, 4] Sigma_B = np.array([     [1.5, 0.0],     [0.0, 1.5] ])  class_A = np.random.multivariate_normal(mu_A, Sigma_A, size=500) class_B = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack((class_A, class_B)) y = np.array([-1]*500 + [1]*500)  print(\"Dataset shape:\", X.shape)  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") plt.legend() plt.show()  <pre>Dataset shape: (1000, 2)\n</pre> In\u00a0[4]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import accuracy_score\n\nw = np.zeros(2, dtype=float)\nb = 0\nalpha = 0.01\n\ndef activation(scores):\n    return np.where(scores &gt;= 0, 1, -1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    \n    wrong = False\n    \n    for index in range(len(X)):\n        \n        x_i = X[index]\n        y_i = y[index]\n\n        score = np.dot(w, x_i) + b\n        \n        y_pred = activation(score)\n        \n        error = y_i - y_pred\n        \n        if error != 0:\n            w += alpha * error * x_i \n            b += alpha * error\n            wrong = True\n\n    if not wrong:            \n        break\n\nscores = X @ w + b\ny_pred = activation(scores)\n\nprint(\"Final weights:\", w, \"bias:\", b)\nprint(\"Accuracy:\", accuracy_score(y, y_pred))\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\")\nplt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nx_plot = np.linspace(0, 10, 100)\ny_plot = -(w[0]/w[1])*x_plot - b/w[1]\nplt.plot(x_plot, y_plot, 'r-')\nplt.legend()\nplt.show()\n</pre> import numpy as np from sklearn.metrics import accuracy_score  w = np.zeros(2, dtype=float) b = 0 alpha = 0.01  def activation(scores):     return np.where(scores &gt;= 0, 1, -1)  epochs = 100  for epoch in range(epochs):          wrong = False          for index in range(len(X)):                  x_i = X[index]         y_i = y[index]          score = np.dot(w, x_i) + b                  y_pred = activation(score)                  error = y_i - y_pred                  if error != 0:             w += alpha * error * x_i              b += alpha * error             wrong = True      if not wrong:                     break  scores = X @ w + b y_pred = activation(scores)  print(\"Final weights:\", w, \"bias:\", b) print(\"Accuracy:\", accuracy_score(y, y_pred))  plt.figure(figsize=(8,6)) plt.scatter(X[y==-1, 0], X[y==-1, 1], alpha=0.6, label=\"Class A\") plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.6, label=\"Class B\") plt.title(\"Dados Sinteticos - Divisao de Classes por Perceptron\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\") x_plot = np.linspace(0, 10, 100) y_plot = -(w[0]/w[1])*x_plot - b/w[1] plt.plot(x_plot, y_plot, 'r-') plt.legend() plt.show() <pre>Final weights: [0.18071183 0.10106285] bias: -0.36000000000000004\nAccuracy: 0.509\n</pre> <p>The data isn't as well separated as in the first exercise. This is made clear by the fact that the classes haev overlap, while in the previous situation they were already separated. This means that, for these two classes, a linear division isn't possible in the way that we're implementing.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/Perceptron/ex2_perceptron/#data-generation-task","title":"Data Generation Task:\u00b6","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <p>Mean =</p> <p>$[2, 2]$</p> <p>Covariance matrix =</p> <p>$[[0.5, 0], [0, 0.5]]$</p> <p>(i.e., variance of $0.5$ along each dimension, no covariance).</p> </li> <li><p>Class 1:</p> <p>Mean =</p> <p>$[5, 5]$</p> <p>Covariance matrix =</p> <p>$[[0.5, 0], [0, 0.5]]$</p> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#perceptron-implementation-task","title":"Perceptron Implementation Task:\u00b6","text":"<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).</li> <li>Use the perceptron learning rule: For each misclassified sample $(x, y)$, update $w = w + \u03b7 * y * x$ and $b = b + \u03b7 * y$, where $\u03b7$ is the learning rate (start with $\u03b7=0.01$).</li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</li> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by $w\u00b7x + b = 0$) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/Perceptron/ex2_perceptron/#data-generation-task","title":"Data Generation Task:\u00b6","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <p>Mean =</p> <p>$[3, 3]$</p> <p>Covariance matrix =</p> <p>$[[1.5, 0], [0, 1.5]]$</p> </li> <li><p>Class 1:</p> <p>Mean =</p> <p>$[4, 4]$</p> <p>Covariance matrix =</p> <p>$[[1.5, 0], [0, 1.5]]$</p> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p>"},{"location":"Exercicios/Perceptron/ex2_perceptron/#perceptron-implementation-task","title":"Perceptron Implementation Task:\u00b6","text":"<p>Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.</p> <ul> <li>Follow the same initialization, update rule, and training process.</li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p>"}]}